{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CMI_T1w_GM-WM-CSFmasks_age_predict.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arokem/IntroDL/blob/master/CMI_T1w_GM_WM_CSFmasks_age_predict.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acu_oYPsF8ik",
        "colab_type": "text"
      },
      "source": [
        "# Saige Rutherford \n",
        "\n",
        ">Research Computer Specialist\n",
        "\n",
        ">Department of Psychiatry, University of Michigan\n",
        "\n",
        "[Website](https://www.beingsaige.com)\n",
        "[Twitter](https://www.twitter.com/being_saige)\n",
        "[GitHub](https://github.com/saigerutherford)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_p56S_8JXZE",
        "colab_type": "text"
      },
      "source": [
        "Data engineering scripts used to prepare this data can be accessed here: \n",
        "\n",
        "[Data Prep Scripts](https://github.com/saigerutherford/anatomically_defined_CNNs)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufve_7o0S5Z8",
        "colab_type": "code",
        "outputId": "a6ef84ea-4668-4452-e65f-1d989111b413",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!wget -nc https://raw.githubusercontent.com/saigerutherford/anatomically_defined_CNNs/master/requirements.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-06-07 12:05:35--  https://raw.githubusercontent.com/saigerutherford/anatomically_defined_CNNs/master/requirements.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1317 (1.3K) [text/plain]\n",
            "Saving to: ‘requirements.txt’\n",
            "\n",
            "\rrequirements.txt      0%[                    ]       0  --.-KB/s               \rrequirements.txt    100%[===================>]   1.29K  --.-KB/s    in 0s      \n",
            "\n",
            "2019-06-07 12:05:35 (223 MB/s) - ‘requirements.txt’ saved [1317/1317]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zutiqJWkUFmk",
        "colab_type": "code",
        "outputId": "36770499-e277-47b9-df9d-af2bb3c42c05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3421
        }
      },
      "source": [
        "!pip install -r requirements.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: absl-py==0.7.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (0.7.1)\n",
            "Requirement already satisfied: astor==0.8.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (0.8.0)\n",
            "Requirement already satisfied: attrs==19.1.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (19.1.0)\n",
            "Requirement already satisfied: backcall==0.1.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 4)) (0.1.0)\n",
            "Collecting bleach==1.5.0 (from -r requirements.txt (line 5))\n",
            "  Downloading https://files.pythonhosted.org/packages/33/70/86c5fec937ea4964184d4d6c4f0b9551564f821e1c3575907639036d9b90/bleach-1.5.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: cycler==0.10.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 6)) (0.10.0)\n",
            "Requirement already satisfied: decorator==4.4.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 7)) (4.4.0)\n",
            "Requirement already satisfied: defusedxml==0.6.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 8)) (0.6.0)\n",
            "Requirement already satisfied: entrypoints==0.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 9)) (0.3)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 10)) (0.2.2)\n",
            "Collecting grpcio==1.21.1 (from -r requirements.txt (line 11))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/83/18f374294bf34128a448ee2fae37651f943b0b5fa473b5b3aff262c15bf8/grpcio-1.21.1-cp36-cp36m-manylinux1_x86_64.whl (2.2MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2MB 7.7MB/s \n",
            "\u001b[?25hCollecting h5py==2.9.0 (from -r requirements.txt (line 12))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/99/d7d4fbf2d02bb30fb76179911a250074b55b852d34e98dd452a9f394ac06/h5py-2.9.0-cp36-cp36m-manylinux1_x86_64.whl (2.8MB)\n",
            "\u001b[K     |████████████████████████████████| 2.8MB 41.5MB/s \n",
            "\u001b[?25hCollecting html5lib==0.9999999 (from -r requirements.txt (line 13))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/ae/bcb60402c60932b32dfaf19bb53870b29eda2cd17551ba5639219fb5ebf9/html5lib-0.9999999.tar.gz (889kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 48.8MB/s \n",
            "\u001b[?25hCollecting ipykernel==5.1.1 (from -r requirements.txt (line 14))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a0/35/dd97fbb48d4e6b5ae97307497e31e46691adc2feedb6279d29fc1c8ad9c1/ipykernel-5.1.1-py3-none-any.whl (114kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 49.8MB/s \n",
            "\u001b[?25hCollecting ipython==7.5.0 (from -r requirements.txt (line 15))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/2e/41dce4ed129057e05a555a7f9629aa2d5f81fdcd4d16568bc24b75a1d2c9/ipython-7.5.0-py3-none-any.whl (770kB)\n",
            "\u001b[K     |████████████████████████████████| 778kB 44.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: ipython-genutils==0.2.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 16)) (0.2.0)\n",
            "Requirement already satisfied: ipywidgets==7.4.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 17)) (7.4.2)\n",
            "Requirement already satisfied: jedi==0.13.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 18)) (0.13.3)\n",
            "Requirement already satisfied: Jinja2==2.10.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 19)) (2.10.1)\n",
            "Requirement already satisfied: joblib==0.13.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 20)) (0.13.2)\n",
            "Collecting jsonschema==3.0.1 (from -r requirements.txt (line 21))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/aa/69/df679dfbdd051568b53c38ec8152a3ab6bc533434fc7ed11ab034bf5e82f/jsonschema-3.0.1-py2.py3-none-any.whl (54kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 28.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: jupyter==1.0.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 22)) (1.0.0)\n",
            "Requirement already satisfied: jupyter-client==5.2.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 23)) (5.2.4)\n",
            "Requirement already satisfied: jupyter-console==6.0.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 24)) (6.0.0)\n",
            "Requirement already satisfied: jupyter-core==4.4.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 25)) (4.4.0)\n",
            "Collecting jupyterlab==0.35.6 (from -r requirements.txt (line 26))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dd/7f/18b4ecfa055243f1eccdb1d7a1cdc0ae529f3df4c1098cee442ad177511a/jupyterlab-0.35.6-py3-none-any.whl (14.8MB)\n",
            "\u001b[K     |████████████████████████████████| 14.8MB 32.1MB/s \n",
            "\u001b[?25hCollecting jupyterlab-server==0.2.0 (from -r requirements.txt (line 27))\n",
            "  Downloading https://files.pythonhosted.org/packages/78/77/e8a9c300afbe24aa46abaf1091d9e7b82328559e99cf2d601e858bcb3e1a/jupyterlab_server-0.2.0-py3-none-any.whl\n",
            "Requirement already satisfied: Keras==2.2.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 28)) (2.2.4)\n",
            "Collecting Keras-Applications==1.0.8 (from -r requirements.txt (line 29))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 25.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: Keras-Preprocessing==1.0.9 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 30)) (1.0.9)\n",
            "Requirement already satisfied: kiwisolver==1.1.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 31)) (1.1.0)\n",
            "Requirement already satisfied: Markdown==3.1.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 32)) (3.1.1)\n",
            "Requirement already satisfied: MarkupSafe==1.1.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 33)) (1.1.1)\n",
            "Requirement already satisfied: matplotlib==3.0.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 34)) (3.0.3)\n",
            "Requirement already satisfied: mistune==0.8.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 35)) (0.8.4)\n",
            "Requirement already satisfied: mock==3.0.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 36)) (3.0.5)\n",
            "Requirement already satisfied: nbconvert==5.5.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 37)) (5.5.0)\n",
            "Requirement already satisfied: nbformat==4.4.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 38)) (4.4.0)\n",
            "Collecting nibabel==2.4.1 (from -r requirements.txt (line 39))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/66/30/fbed62172920c3fd050b6483541546a87c5e735f4a0ef03f08bb150680b4/nibabel-2.4.1-py2.py3-none-any.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 32.5MB/s \n",
            "\u001b[?25hCollecting nilearn==0.5.2 (from -r requirements.txt (line 40))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6e/65/ba76e7cd544dafc28960e60b099d6f906a2096034c560158beaf2ff299bc/nilearn-0.5.2-py2.py3-none-any.whl (2.3MB)\n",
            "\u001b[K     |████████████████████████████████| 2.4MB 30.1MB/s \n",
            "\u001b[?25hCollecting notebook==5.7.8 (from -r requirements.txt (line 41))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/36/89ebfffc9dd8c8dbd81c1ffb53e3d4233ee666414c143959477cb07cc5f5/notebook-5.7.8-py2.py3-none-any.whl (9.0MB)\n",
            "\u001b[K     |████████████████████████████████| 9.0MB 32.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy==1.16.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 42)) (1.16.4)\n",
            "Requirement already satisfied: pandas==0.24.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 43)) (0.24.2)\n",
            "Requirement already satisfied: pandocfilters==1.4.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 44)) (1.4.2)\n",
            "Requirement already satisfied: parso==0.4.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 45)) (0.4.0)\n",
            "Requirement already satisfied: pexpect==4.7.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 46)) (4.7.0)\n",
            "Requirement already satisfied: pickleshare==0.7.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 47)) (0.7.5)\n",
            "Requirement already satisfied: prometheus-client==0.6.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 48)) (0.6.0)\n",
            "Collecting prompt-toolkit==2.0.9 (from -r requirements.txt (line 49))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/a7/9b1dd14ef45345f186ef69d175bdd2491c40ab1dfa4b2b3e4352df719ed7/prompt_toolkit-2.0.9-py3-none-any.whl (337kB)\n",
            "\u001b[K     |████████████████████████████████| 337kB 46.4MB/s \n",
            "\u001b[?25hCollecting protobuf==3.8.0 (from -r requirements.txt (line 50))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d2/fb/29de8d08967f0cce1bb10b39846d836b0f3bf6776ddc36aed7c73498ca7e/protobuf-3.8.0-cp36-cp36m-manylinux1_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 41.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: ptyprocess==0.6.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 51)) (0.6.0)\n",
            "Collecting pydot==1.4.1 (from -r requirements.txt (line 52))\n",
            "  Downloading https://files.pythonhosted.org/packages/33/d1/b1479a770f66d962f545c2101630ce1d5592d90cb4f083d38862e93d16d2/pydot-1.4.1-py2.py3-none-any.whl\n",
            "Collecting Pygments==2.4.2 (from -r requirements.txt (line 53))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/73/1dfa428150e3ccb0fa3e68db406e5be48698f2a979ccbcec795f28f44048/Pygments-2.4.2-py2.py3-none-any.whl (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 42.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing==2.4.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 54)) (2.4.0)\n",
            "Requirement already satisfied: pyrsistent==0.15.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 55)) (0.15.2)\n",
            "Collecting python-dateutil==2.8.0 (from -r requirements.txt (line 56))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/41/17/c62faccbfbd163c7f57f3844689e3a78bae1f403648a6afb1d0866d87fbb/python_dateutil-2.8.0-py2.py3-none-any.whl (226kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 54.1MB/s \n",
            "\u001b[?25hCollecting pytz==2019.1 (from -r requirements.txt (line 57))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3d/73/fe30c2daaaa0713420d0382b16fbb761409f532c56bdcc514bf7b6262bb6/pytz-2019.1-py2.py3-none-any.whl (510kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 52.8MB/s \n",
            "\u001b[?25hCollecting PyYAML==5.1 (from -r requirements.txt (line 58))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9f/2c/9417b5c774792634834e730932745bc09a7d36754ca00acf1ccd1ac2594d/PyYAML-5.1.tar.gz (274kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 41.2MB/s \n",
            "\u001b[?25hCollecting pyzmq==18.0.1 (from -r requirements.txt (line 59))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5f/04/f6f0fa20b698b29c6e6b1d6b4b575c12607b0abf61810aab1df4099988c6/pyzmq-18.0.1-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 43.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: qtconsole==4.5.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 60)) (4.5.1)\n",
            "Requirement already satisfied: scikit-learn==0.21.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 61)) (0.21.2)\n",
            "Requirement already satisfied: scipy==1.3.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 62)) (1.3.0)\n",
            "Requirement already satisfied: Send2Trash==1.5.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 63)) (1.5.0)\n",
            "Requirement already satisfied: six==1.12.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 64)) (1.12.0)\n",
            "Requirement already satisfied: tensorboard==1.13.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 65)) (1.13.1)\n",
            "Requirement already satisfied: tensorflow-estimator==1.13.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 66)) (1.13.0)\n",
            "Collecting tensorflow-gpu==1.13.1 (from -r requirements.txt (line 67))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7b/b1/0ad4ae02e17ddd62109cd54c291e311c4b5fd09b4d0678d3d6ce4159b0f0/tensorflow_gpu-1.13.1-cp36-cp36m-manylinux1_x86_64.whl (345.2MB)\n",
            "\u001b[K     |████████████████████████████████| 345.2MB 83kB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor==1.1.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 68)) (1.1.0)\n",
            "Requirement already satisfied: terminado==0.8.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 69)) (0.8.2)\n",
            "Requirement already satisfied: testpath==0.4.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 70)) (0.4.2)\n",
            "Collecting tornado==6.0.2 (from -r requirements.txt (line 71))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/03/3f/5f89d99fca3c0100c8cede4f53f660b126d39e0d6a1e943e95cc3ed386fb/tornado-6.0.2.tar.gz (481kB)\n",
            "\u001b[K     |████████████████████████████████| 491kB 46.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: traitlets==4.3.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 72)) (4.3.2)\n",
            "Requirement already satisfied: wcwidth==0.1.7 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 73)) (0.1.7)\n",
            "Requirement already satisfied: webencodings==0.5.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 74)) (0.5.1)\n",
            "Requirement already satisfied: Werkzeug==0.15.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 75)) (0.15.4)\n",
            "Requirement already satisfied: widgetsnbextension==3.4.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 76)) (3.4.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython==7.5.0->-r requirements.txt (line 15)) (41.0.1)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard==1.13.1->-r requirements.txt (line 65)) (0.33.4)\n",
            "Building wheels for collected packages: html5lib, PyYAML, tornado\n",
            "  Building wheel for html5lib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/ae/f9/d2b189788efcf61d1ee0e36045476735c838898eef1cad6e29\n",
            "  Building wheel for PyYAML (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/ad/56/bc/1522f864feb2a358ea6f1a92b4798d69ac783a28e80567a18b\n",
            "  Building wheel for tornado (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/61/7e/7a/5e02e60dc329aef32ecf70e0425319ee7e2198c3a7cf98b4a2\n",
            "Successfully built html5lib PyYAML tornado\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement ipykernel~=4.6.0, but you'll have ipykernel 5.1.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement ipython~=5.5.0, but you'll have ipython 7.5.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement notebook~=5.2.0, but you'll have notebook 5.7.8 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement tornado~=4.5.0, but you'll have tornado 6.0.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: html5lib, bleach, grpcio, h5py, prompt-toolkit, Pygments, ipython, tornado, ipykernel, jsonschema, pyzmq, notebook, jupyterlab-server, jupyterlab, Keras-Applications, nibabel, nilearn, protobuf, pydot, python-dateutil, pytz, PyYAML, tensorflow-gpu\n",
            "  Found existing installation: html5lib 1.0.1\n",
            "    Uninstalling html5lib-1.0.1:\n",
            "      Successfully uninstalled html5lib-1.0.1\n",
            "  Found existing installation: bleach 3.1.0\n",
            "    Uninstalling bleach-3.1.0:\n",
            "      Successfully uninstalled bleach-3.1.0\n",
            "  Found existing installation: grpcio 1.15.0\n",
            "    Uninstalling grpcio-1.15.0:\n",
            "      Successfully uninstalled grpcio-1.15.0\n",
            "  Found existing installation: h5py 2.8.0\n",
            "    Uninstalling h5py-2.8.0:\n",
            "      Successfully uninstalled h5py-2.8.0\n",
            "  Found existing installation: prompt-toolkit 1.0.16\n",
            "    Uninstalling prompt-toolkit-1.0.16:\n",
            "      Successfully uninstalled prompt-toolkit-1.0.16\n",
            "  Found existing installation: Pygments 2.1.3\n",
            "    Uninstalling Pygments-2.1.3:\n",
            "      Successfully uninstalled Pygments-2.1.3\n",
            "  Found existing installation: ipython 5.5.0\n",
            "    Uninstalling ipython-5.5.0:\n",
            "      Successfully uninstalled ipython-5.5.0\n",
            "  Found existing installation: tornado 4.5.3\n",
            "    Uninstalling tornado-4.5.3:\n",
            "      Successfully uninstalled tornado-4.5.3\n",
            "  Found existing installation: ipykernel 4.6.1\n",
            "    Uninstalling ipykernel-4.6.1:\n",
            "      Successfully uninstalled ipykernel-4.6.1\n",
            "  Found existing installation: jsonschema 2.6.0\n",
            "    Uninstalling jsonschema-2.6.0:\n",
            "      Successfully uninstalled jsonschema-2.6.0\n",
            "  Found existing installation: pyzmq 17.0.0\n",
            "    Uninstalling pyzmq-17.0.0:\n",
            "      Successfully uninstalled pyzmq-17.0.0\n",
            "  Found existing installation: notebook 5.2.2\n",
            "    Uninstalling notebook-5.2.2:\n",
            "      Successfully uninstalled notebook-5.2.2\n",
            "  Found existing installation: Keras-Applications 1.0.7\n",
            "    Uninstalling Keras-Applications-1.0.7:\n",
            "      Successfully uninstalled Keras-Applications-1.0.7\n",
            "  Found existing installation: nibabel 2.3.3\n",
            "    Uninstalling nibabel-2.3.3:\n",
            "      Successfully uninstalled nibabel-2.3.3\n",
            "  Found existing installation: protobuf 3.7.1\n",
            "    Uninstalling protobuf-3.7.1:\n",
            "      Successfully uninstalled protobuf-3.7.1\n",
            "  Found existing installation: pydot 1.3.0\n",
            "    Uninstalling pydot-1.3.0:\n",
            "      Successfully uninstalled pydot-1.3.0\n",
            "  Found existing installation: python-dateutil 2.5.3\n",
            "    Uninstalling python-dateutil-2.5.3:\n",
            "      Successfully uninstalled python-dateutil-2.5.3\n",
            "  Found existing installation: pytz 2018.9\n",
            "    Uninstalling pytz-2018.9:\n",
            "      Successfully uninstalled pytz-2018.9\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed Keras-Applications-1.0.8 PyYAML-5.1 Pygments-2.4.2 bleach-1.5.0 grpcio-1.21.1 h5py-2.9.0 html5lib-0.9999999 ipykernel-5.1.1 ipython-7.5.0 jsonschema-3.0.1 jupyterlab-0.35.6 jupyterlab-server-0.2.0 nibabel-2.4.1 nilearn-0.5.2 notebook-5.7.8 prompt-toolkit-2.0.9 protobuf-3.8.0 pydot-1.4.1 python-dateutil-2.8.0 pytz-2019.1 pyzmq-18.0.1 tensorflow-gpu-1.13.1 tornado-6.0.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "IPython",
                  "dateutil",
                  "google",
                  "grpc",
                  "ipykernel",
                  "prompt_toolkit",
                  "pygments",
                  "pytz",
                  "tornado",
                  "zmq"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nLVyU7QTV8z8"
      },
      "source": [
        "---\n",
        "## Step 0: Load & Setup The Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TqLSFI6dV8z-",
        "outputId": "473a4e12-7a00-4f64-ac93-551019cf7aea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# general python packages\n",
        "from __future__ import print_function\n",
        "import os \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random as ran\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Deep Learning packages\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Conv3D, MaxPooling3D, AveragePooling3D, Input, ZeroPadding3D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras import initializers\n",
        "from keras.callbacks import ModelCheckpoint, TensorBoard, CSVLogger, EarlyStopping, ReduceLROnPlateau\n",
        "from keras.optimizers import Adam, Adadelta, RMSprop, Adamax, Nadam, SGD # using Adam in this model, but just showing the other options here for future reference\n",
        "from keras import backend as K\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IqStSb7GJQM-",
        "outputId": "a7b6c556-9362-4a4d-e72d-bf81049bcd32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# neuroimaging-specific python packages\n",
        "import nilearn\n",
        "from nilearn import plotting\n",
        "import nibabel as nib"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
            "  warnings.warn(msg, category=DeprecationWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhdvos4sVFc8",
        "colab_type": "code",
        "outputId": "7946b346-271b-4628-bb4a-f8fadb298d70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!wget -nc https://raw.githubusercontent.com/saigerutherford/anatomically_defined_CNNs/master/data/pheno_file.csv"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-06-07 12:07:58--  https://raw.githubusercontent.com/saigerutherford/anatomically_defined_CNNs/master/data/pheno_file.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 51946 (51K) [text/plain]\n",
            "Saving to: ‘pheno_file.csv’\n",
            "\n",
            "\rpheno_file.csv        0%[                    ]       0  --.-KB/s               \rpheno_file.csv      100%[===================>]  50.73K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2019-06-07 12:07:58 (3.32 MB/s) - ‘pheno_file.csv’ saved [51946/51946]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "g1ONeN50yrMR",
        "colab": {}
      },
      "source": [
        "# Read in csv file with subject info\n",
        "pheno = pd.read_csv('pheno_file.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KoATQCS98sGJ",
        "outputId": "31258f0f-702c-4195-fea8-ef11a71c2636",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "pheno['Age'].describe()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    906.000000\n",
              "mean      10.824074\n",
              "std        3.558937\n",
              "min        5.036048\n",
              "25%        8.039898\n",
              "50%       10.031599\n",
              "75%       13.057266\n",
              "max       21.816563\n",
              "Name: Age, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6GFHRlLVSru",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "E7s_ypgF-ONi",
        "outputId": "643109c5-6359-4671-d175-b1111d4b04ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        }
      },
      "source": [
        "plt.figure(figsize=(6,6))\n",
        "plt.hist(pheno['Age'],bins=15)\n",
        "plt.show"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function matplotlib.pyplot.show>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAFpCAYAAAB0yyjhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBhn+ktQgw1+SGmT4S1KD/g9w2KagrzlRMwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBNL48Tn_hO-",
        "colab_type": "code",
        "outputId": "c3cb3cdf-3f5d-4190-ecf2-049aff1eb276",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!wget -nc https://storage.googleapis.com/ohbm-dl-lindsay-data/OHBM_DL_SR_data.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-06-07 12:08:07--  https://storage.googleapis.com/ohbm-dl-lindsay-data/OHBM_DL_SR_data.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.195.128, 2607:f8b0:400e:c09::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.195.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 265718959 (253M) [application/zip]\n",
            "Saving to: ‘OHBM_DL_SR_data.zip’\n",
            "\n",
            "OHBM_DL_SR_data.zip 100%[===================>] 253.41M   174MB/s    in 1.5s    \n",
            "\n",
            "2019-06-07 12:08:09 (174 MB/s) - ‘OHBM_DL_SR_data.zip’ saved [265718959/265718959]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zZDkxQe_tgU",
        "colab_type": "code",
        "outputId": "ea866a91-eb6e-41b6-e6c8-cd43c99782c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "!unzip -uo OHBM_DL_SR_data.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  OHBM_DL_SR_data.zip\n",
            "  inflating: OHBM_DL_data/downsampled_test_labels.npy  \n",
            "  inflating: OHBM_DL_data/savedmodels/metadata_batch20_epoch10.tsv  \n",
            "  inflating: OHBM_DL_data/downsampled_train_labels.npy  \n",
            "  inflating: OHBM_DL_data/savedmodels/best_weights_batch5epoch30.h5  \n",
            "  inflating: OHBM_DL_data/savedmodels/log_batch20epoch10.csv  \n",
            "  inflating: OHBM_DL_data/savedmodels/weights.20-12.44.hdf5  \n",
            "  inflating: OHBM_DL_data/savedmodels/best_weights_batch20epoch10.h5  \n",
            "  inflating: OHBM_DL_data/savedmodels/metadata_batch5_epoch30.tsv  \n",
            "  inflating: OHBM_DL_data/savedmodels/metadata_batch10_epoch20.tsv  \n",
            "  inflating: OHBM_DL_data/savedmodels/log_batch10epoch20.csv  \n",
            "  inflating: OHBM_DL_data/savedmodels/SavedModel_batch20_epoch10.h5  \n",
            "  inflating: OHBM_DL_data/savedmodels/SavedModel_batch10_epoch20.h5  \n",
            "  inflating: OHBM_DL_data/savedmodels/SavedModel_batch5_epoch30.h5  \n",
            "  inflating: OHBM_DL_data/savedmodels/TensorBoardLogs/events.out.tfevents.1559899998.freewill  \n",
            "  inflating: OHBM_DL_data/savedmodels/best_weights_batch10epoch20.h5  \n",
            "  inflating: OHBM_DL_data/savedmodels/TensorBoardLogs/events.out.tfevents.1559900982.freewill  \n",
            "  inflating: OHBM_DL_data/savedmodels/TensorBoardLogs/events.out.tfevents.1559901556.freewill  \n",
            "  inflating: OHBM_DL_data/downsampled_test.npy  \n",
            "  inflating: OHBM_DL_data/savedmodels/TensorBoardLogs/events.out.tfevents.1559899954.freewill  \n",
            "  inflating: OHBM_DL_data/downsampled_train.npy  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "590v94VWMvMC",
        "colab": {}
      },
      "source": [
        "root_path = 'OHBM_DL_data/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iC9a7yMekY9M",
        "colab_type": "code",
        "outputId": "28e5214c-e7aa-43d0-f898-85147b544af2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "ls OHBM_DL_data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downsampled_test_labels.npy  downsampled_train_labels.npy  \u001b[0m\u001b[01;34msavedmodels\u001b[0m/\n",
            "downsampled_test.npy         downsampled_train.npy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3eoXphLS4Vs",
        "colab_type": "text"
      },
      "source": [
        "# Load data (already divided into train/test sets)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgixVX64dz5y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = np.load(root_path + 'downsampled_train.npy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXf0BXdBeO27",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_test = np.load(root_path + 'downsampled_test.npy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0VjqB-leXm1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train = np.load(root_path + 'downsampled_train_labels.npy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DIgNJ2BefLP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_test = np.load(root_path + 'downsampled_test_labels.npy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2OtdFmCS4Vn",
        "colab_type": "text"
      },
      "source": [
        "# Check input shape\n",
        "### X_train.shape[0] = batch size (number of subjects in training)\n",
        "### X_train.shape[1] = rows (x)\n",
        "### X_train.shape[2] = columns (y)\n",
        "### X_train.shape[3] = depth (z)\n",
        "### X_train.shape[4] = channels (number of segmentations)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUabq7k0HF1B",
        "colab_type": "code",
        "outputId": "2147ac86-c3cd-401b-ff7f-f27b50cafd8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(724, 48, 60, 46, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZG4sgLJfV80r",
        "colab": {}
      },
      "source": [
        "n_train = X_train.shape[0]\n",
        "n_test = X_test.shape[0]\n",
        "image_shape = X_train[0].shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xxu1qZ1MV80u",
        "outputId": "b3247137-f3ed-40d8-a684-939008f97dab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(\"Number of training examples =\", n_train)\n",
        "print (\"Number of test examples\", n_test)\n",
        "print(\"Image data shape =\", image_shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training examples = 724\n",
            "Number of test examples 182\n",
            "Image data shape = (48, 60, 46, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "COQla7JGWYps"
      },
      "source": [
        "# Step 1: Model Construction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_9hiCC2oV81k",
        "scrolled": false,
        "outputId": "c7a59535-0c27-4eae-c479-0d26a9be7413",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 921
        }
      },
      "source": [
        "model = Sequential() # The simplest model, a linear stack of layers\n",
        "\n",
        "model.add(Conv3D(filters=64,\n",
        "                 kernel_size=(3,3,3), #determines the width, height, depth of the 3D convolution window\n",
        "                 activation='elu', #Exponential Linear Unit\n",
        "                 strides=1,\n",
        "                 padding='same',\n",
        "                 kernel_initializer='glorot_uniform', \n",
        "                 input_shape=image_shape)) # only the first layer needs to be told this info\n",
        "model.add(Conv3D(filters=64, kernel_size=(3,3,3), activation='elu', strides=(1,1,1), padding='same'))\n",
        "model.add(Conv3D(filters=64, kernel_size=(3,3,3), activation='elu', strides=(1,1,1), padding='same'))\n",
        "model.add(MaxPooling3D((2,2,2),strides=(2,2,2))) # pooling is also referred to as a downsampling layer\n",
        "model.add(BatchNormalization()) # Normalize the activations of the previous layer at each batch (aka make the mean activation close to 0 and the activation standard deviation close to 1)\n",
        "\n",
        "model.add(Conv3D(filters=32, kernel_size=(3,3,3), activation='elu', strides=(1,1,1), padding='same'))\n",
        "model.add(Conv3D(filters=32, kernel_size=(3,3,3), activation='elu', strides=(1,1,1), padding='same'))\n",
        "model.add(MaxPooling3D((2,2,2),strides=(2,2,2)))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv3D(filters=16, kernel_size=(3,3,3), activation='elu', strides=(1,1,1), padding='same'))\n",
        "model.add(Conv3D(filters=16, kernel_size=(3,3,3), activation='elu', strides=(1,1,1), padding='same'))\n",
        "model.add(MaxPooling3D((2,2,2),strides=(2,2,2)))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv3D(filters=8, kernel_size=(3,3,3), activation='elu', strides=(1,1,1), padding='same'))\n",
        "model.add(Conv3D(filters=8, kernel_size=(3,3,3), activation='elu', strides=(1,1,1), padding='same'))\n",
        "model.add(MaxPooling3D((2,2,2),strides=(2,2,2)))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(AveragePooling3D((2,2,2),strides=(2,2,2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1024, activation='relu',name='features')) #convert the output of the convolutional part of the CNN into a 1D feature vector. Length of vector = n_classes\n",
        "model.add(Dense(1)) # final output is a single number (Age in this model)\n",
        "model.summary()\n",
        "\n",
        "filename=\"best_weights.h5\"\n",
        "filename2=\"weights.{epoch:02d}-{val_loss:.2f}.hdf5\"\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv3d_1 (Conv3D)            (None, 48, 60, 46, 64)    5248      \n",
            "_________________________________________________________________\n",
            "conv3d_2 (Conv3D)            (None, 48, 60, 46, 64)    110656    \n",
            "_________________________________________________________________\n",
            "conv3d_3 (Conv3D)            (None, 48, 60, 46, 64)    110656    \n",
            "_________________________________________________________________\n",
            "max_pooling3d_1 (MaxPooling3 (None, 24, 30, 23, 64)    0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 24, 30, 23, 64)    256       \n",
            "_________________________________________________________________\n",
            "conv3d_4 (Conv3D)            (None, 24, 30, 23, 32)    55328     \n",
            "_________________________________________________________________\n",
            "conv3d_5 (Conv3D)            (None, 24, 30, 23, 32)    27680     \n",
            "_________________________________________________________________\n",
            "max_pooling3d_2 (MaxPooling3 (None, 12, 15, 11, 32)    0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 12, 15, 11, 32)    128       \n",
            "_________________________________________________________________\n",
            "conv3d_6 (Conv3D)            (None, 12, 15, 11, 16)    13840     \n",
            "_________________________________________________________________\n",
            "conv3d_7 (Conv3D)            (None, 12, 15, 11, 16)    6928      \n",
            "_________________________________________________________________\n",
            "max_pooling3d_3 (MaxPooling3 (None, 6, 7, 5, 16)       0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 6, 7, 5, 16)       64        \n",
            "_________________________________________________________________\n",
            "conv3d_8 (Conv3D)            (None, 6, 7, 5, 8)        3464      \n",
            "_________________________________________________________________\n",
            "conv3d_9 (Conv3D)            (None, 6, 7, 5, 8)        1736      \n",
            "_________________________________________________________________\n",
            "max_pooling3d_4 (MaxPooling3 (None, 3, 3, 2, 8)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 3, 3, 2, 8)        32        \n",
            "_________________________________________________________________\n",
            "average_pooling3d_1 (Average (None, 1, 1, 1, 8)        0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "features (Dense)             (None, 1024)              9216      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 1025      \n",
            "=================================================================\n",
            "Total params: 346,257\n",
            "Trainable params: 346,017\n",
            "Non-trainable params: 240\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fVOUv2emV81q",
        "colab": {}
      },
      "source": [
        "checkpoints = []\n",
        "\n",
        "if not os.path.exists('Results01/'):\n",
        "    os.makedirs('Results01/')\n",
        "\n",
        "checkpoints.append(ModelCheckpoint('Results01/'+filename, \n",
        "                                   monitor='val_loss', \n",
        "                                   verbose=1, \n",
        "                                   save_best_only=True, \n",
        "                                   save_weights_only=True, \n",
        "                                   mode='auto', \n",
        "                                   period=1))\n",
        "\n",
        "checkpoints.append(ModelCheckpoint('Results01/'+filename2, \n",
        "                                   monitor='val_loss', \n",
        "                                   verbose=1, \n",
        "                                   save_best_only=False, \n",
        "                                   save_weights_only=True, \n",
        "                                   mode='auto', \n",
        "                                   period=20))\n",
        "\n",
        "checkpoints.append(TensorBoard(log_dir='Results01/TensorBoardLogs', \n",
        "                               histogram_freq=0, \n",
        "                               write_graph=True, \n",
        "                               write_images=False, \n",
        "                               embeddings_freq=0, \n",
        "                               embeddings_layer_names=['features'], \n",
        "                               embeddings_metadata='metadata.tsv'))\n",
        "#Early Stopping here is set so that if the MSE in the validation set does not improve after 10 epochs, training will stop\n",
        "checkpoints.append(EarlyStopping(monitor='val_loss', mode='auto', min_delta=0, patience=10))\n",
        "checkpoints.append(ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0))\n",
        "checkpoints.append(CSVLogger('Results01/log.csv'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0t1hNJ5FV81t",
        "colab": {}
      },
      "source": [
        "model.compile(loss='mse', # the objective that the model will try to minimize, Mean Square Error in this moel\n",
        "              optimizer='adam', \n",
        "              metrics=['mae']) # add in any other metrics you want to use to show performance of the model, add accuracy here if you're doing classification"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fLQOan47WnC2"
      },
      "source": [
        "# Step 2: Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dwgf9p71S4WD",
        "colab_type": "code",
        "outputId": "e5daeb28-63ac-4df7-f85c-a914404b12e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Check available GPUs\n",
        "K.tensorflow_backend._get_available_gpus()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/job:localhost/replica:0/task:0/device:GPU:0']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "U0OjFqxgV81y",
        "scrolled": false,
        "outputId": "52dd3b74-7619-4c9a-e00f-60ace3907b02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 785
        }
      },
      "source": [
        "NUM_EPOCHS = 10 # defines for how many times the training will repeat. 1 epoch is 1 forward pass and 1 backward pass over all the training examples\n",
        "BATCH_SIZE= 20 # the number of training examples in one forward/backward pass (or for 1 epoch)\n",
        "history1= model.fit(X_train, y_train, \n",
        "          validation_split = 0.1, # This sets how much of the training data should be used as the validation set (test set during training)\n",
        "          batch_size = BATCH_SIZE, \n",
        "          epochs = NUM_EPOCHS,\n",
        "          callbacks = checkpoints)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 651 samples, validate on 73 samples\n",
            "Epoch 1/10\n",
            "651/651 [==============================] - 59s 90ms/step - loss: 57.7585 - mean_absolute_error: 6.2414 - val_loss: 12.6858 - val_mean_absolute_error: 3.0295\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 12.68576, saving model to Results01/best_weights.h5\n",
            "Epoch 2/10\n",
            "651/651 [==============================] - 47s 73ms/step - loss: 13.4328 - mean_absolute_error: 3.0389 - val_loss: 47.6312 - val_mean_absolute_error: 6.0403\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 12.68576\n",
            "Epoch 3/10\n",
            "651/651 [==============================] - 49s 76ms/step - loss: 13.0542 - mean_absolute_error: 2.9412 - val_loss: 111.2868 - val_mean_absolute_error: 9.8150\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 12.68576\n",
            "Epoch 4/10\n",
            "651/651 [==============================] - 50s 76ms/step - loss: 12.6467 - mean_absolute_error: 2.9328 - val_loss: 93.3161 - val_mean_absolute_error: 8.8069\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 12.68576\n",
            "Epoch 5/10\n",
            "651/651 [==============================] - 50s 77ms/step - loss: 12.6467 - mean_absolute_error: 2.9263 - val_loss: 12.8191 - val_mean_absolute_error: 3.0345\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 12.68576\n",
            "Epoch 6/10\n",
            "651/651 [==============================] - 50s 76ms/step - loss: 12.7252 - mean_absolute_error: 2.9031 - val_loss: 137.3590 - val_mean_absolute_error: 11.1460\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 12.68576\n",
            "Epoch 7/10\n",
            "651/651 [==============================] - 50s 77ms/step - loss: 12.6588 - mean_absolute_error: 2.9086 - val_loss: 13.3783 - val_mean_absolute_error: 2.9414\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 12.68576\n",
            "Epoch 8/10\n",
            "651/651 [==============================] - 50s 76ms/step - loss: 12.7859 - mean_absolute_error: 2.9173 - val_loss: 23.4969 - val_mean_absolute_error: 3.7813\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 12.68576\n",
            "Epoch 9/10\n",
            "651/651 [==============================] - 50s 77ms/step - loss: 12.6636 - mean_absolute_error: 2.9246 - val_loss: 31.7347 - val_mean_absolute_error: 4.5825\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 12.68576\n",
            "Epoch 10/10\n",
            "651/651 [==============================] - 50s 77ms/step - loss: 12.6109 - mean_absolute_error: 2.9182 - val_loss: 16.5694 - val_mean_absolute_error: 3.1439\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 12.68576\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8bJ8tQWMpx_8"
      },
      "source": [
        "# Step 3: Model Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3r6PIHp4p1j_",
        "outputId": "6f6a249d-6737-449d-a693-85c3953f9318",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "model.evaluate(x=X_test, y=y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "182/182 [==============================] - 8s 45ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[14.294641683389852, 2.837204506109049]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qeszU1NTp1-c"
      },
      "source": [
        "# Step 4: Save Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZAaugGfNqGbp"
      },
      "source": [
        "**Saving/loading whole models (architecture + weights + optimizer state):**\n",
        "1. the architecture of the model, allowing to re-create the model\n",
        "2. the weights of the model\n",
        "3. the training configuration (loss, optimizer)\n",
        "4. the state of the optimizer, allowing to resume training exactly where you left off\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3vrVuahS4WR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('Results01/SavedModel_Batch-20_Epochs-10.h5') #Always change this name when re-training (if you haven't changed directories) so you don't overwrite!"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3A0USn3KS4WV",
        "colab_type": "text"
      },
      "source": [
        "# Step 5: Load Pre-Trained Model, Compile, Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjLTWnruS4WX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import load_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2JRyNRAS4Wa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pretrain_model_b10e20 = load_model(root_path + 'savedmodels/SavedModel_batch10_epoch20.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FnA2mIPJ5B8e",
        "colab": {}
      },
      "source": [
        "pretrain_model_b5e30 = load_model(root_path + 'savedmodels/SavedModel_batch5_epoch30.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxBsKVC5S4Wb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#pretrain_model_b10e20.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZY2j80CO5Pvt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#pretrain_model_b5e30.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "irrUCpPUiFwx",
        "colab": {}
      },
      "source": [
        "pretrain_model_b10e20.compile(loss='mse', # the objective that the model will try to minimize\n",
        "              optimizer='adam', \n",
        "              metrics=['mae', 'acc']) # add in any other metrics you want to use to show performance of the model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnZF8TMo5UBm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pretrain_model_b5e30.compile(loss='mse', # the objective that the model will try to minimize\n",
        "              optimizer='adam', \n",
        "              metrics=['mae', 'acc']) # add in any other metrics you want to use to show performance of the model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "fa340e56-8bf1-40cd-c493-0af8ef7eb917",
        "id": "-bYxElBXh9Fh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "pretrain_model_b10e20.evaluate(x=X_test, y=y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "182/182 [==============================] - 4s 23ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[12.952054757338304, 2.9791012019901486, 0.0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ix466DE346ip",
        "colab_type": "code",
        "outputId": "f1797f86-cee2-48f2-dc70-3c23b4fcb3cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "pretrain_model_b5e30.evaluate(x=X_test, y=y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "182/182 [==============================] - 5s 25ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[18.971883417485834, 3.136493648801531, 0.0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    }
  ]
}